{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d19953f",
   "metadata": {
    "papermill": {
     "duration": 0.005132,
     "end_time": "2024-11-17T10:34:10.211103",
     "exception": false,
     "start_time": "2024-11-17T10:34:10.205971",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Datast 2 : Digit Recognizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8aaf8b",
   "metadata": {
    "papermill": {
     "duration": 0.004292,
     "end_time": "2024-11-17T10:34:10.220073",
     "exception": false,
     "start_time": "2024-11-17T10:34:10.215781",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this dataset we will explore how our plumber optimizer performs on multiclass classification task with a relatively small dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22490a9e",
   "metadata": {
    "papermill": {
     "duration": 0.004214,
     "end_time": "2024-11-17T10:34:10.228719",
     "exception": false,
     "start_time": "2024-11-17T10:34:10.224505",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Importing the libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "776a8693",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T10:34:10.238946Z",
     "iopub.status.busy": "2024-11-17T10:34:10.238601Z",
     "iopub.status.idle": "2024-11-17T10:34:12.295402Z",
     "shell.execute_reply": "2024-11-17T10:34:12.294596Z"
    },
    "papermill": {
     "duration": 2.064523,
     "end_time": "2024-11-17T10:34:12.297685",
     "exception": false,
     "start_time": "2024-11-17T10:34:10.233162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f29e2888",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T10:34:12.308772Z",
     "iopub.status.busy": "2024-11-17T10:34:12.308377Z",
     "iopub.status.idle": "2024-11-17T10:34:12.533807Z",
     "shell.execute_reply": "2024-11-17T10:34:12.533041Z"
    },
    "papermill": {
     "duration": 0.233575,
     "end_time": "2024-11-17T10:34:12.535971",
     "exception": false,
     "start_time": "2024-11-17T10:34:12.302396",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\"\"\"\n",
    "Random Forest Hyperparameter Optimization\n",
    "\n",
    "Description:\n",
    "    This file implements a technique called the 'Plumber Optimizer', which provides automated tuning for RandomForest models.\n",
    "    The core idea divides the tuning process into two phases: a general and exploratory phase, followed by a second, more focused phase.\n",
    "    It also implements an early stopping technique if a significant number of trials pass with no improvement.\n",
    "    The optimizer allows the user to choose between F1 and accuracy as the metric score for optimization in classification tasks,\n",
    "    while using MSE for regression tasks.\n",
    "\n",
    "Usage:\n",
    "    By providing the training data, specifying whether it's a classification or a regression task, and choosing if F1 should be used\n",
    "    instead of accuracy, the Plumber Optimizer will automatically suggest a number of trials that balance performance and efficiency.\n",
    "    The tuning process begins using the Optuna library. In addition to fully automated tuning, a visualization function is available\n",
    "    that provides diagrams to help users visualize what happened behind the scenes.\n",
    "\n",
    "Authors:\n",
    "    Mohammed Aldahmani\n",
    "    Ali Al-Ali\n",
    "    Abdalla Alzaabi\n",
    "\n",
    "Date:\n",
    "    November 14, 2024\n",
    "\n",
    "Dependencies:\n",
    "    - optuna\n",
    "    - numpy\n",
    "    - pandas\n",
    "    - sklearn\n",
    "    - matplotlib\n",
    "    - psutil\n",
    "\"\"\"\n",
    "\n",
    "class PlumberOptimizer:\n",
    "\n",
    "    def __init__(self, X, y, classification=True, f1=False, verbose=0):\n",
    "        \"\"\"\n",
    "        Constructor for PlumberOptimizer class.\n",
    "        Args:\n",
    "            X: Training data.\n",
    "            y: Target vector.\n",
    "            classification (bool): Set to True if the task is a classification task, False for regression.\n",
    "            f1 (bool): Set to True to use F1 score as the evaluation metric for classification tasks.\n",
    "            verbose (int): If set to 0, suppresses logging; otherwise, Optuna will print the results of each trial.\n",
    "        \n",
    "        Initializes the class by training a single RandomForest model to measure the time it takes, then automatically calculates\n",
    "        a suitable number of trials based on this single model's training time and prints the result to the user.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.best_param = None\n",
    "        self.importance = None\n",
    "        self.study = None\n",
    "        self.improved_study = None\n",
    "        self.best_value = float('-inf') \n",
    "        self.trials_since_last_improvement = 0\n",
    "        self.classification = classification\n",
    "        self.f1 = f1\n",
    "        self.num_trials = self.compute_num_trials()\n",
    "        print(f\"Suggested number of trails is {self.num_trials} \")\n",
    "        if verbose==0:\n",
    "            optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def compute_single_model_time(self):\n",
    "        \"\"\"\n",
    "        Helper function to measure the duration to train a single RandomForest model.\n",
    "        Returns:\n",
    "            float: Time in seconds it takes to train the model.\n",
    "        \"\"\"\n",
    "        model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "        start_time = time.time()\n",
    "        model.fit(self.X, self.y)\n",
    "        return time.time() - start_time\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    def compute_num_trials(self):\n",
    "        \"\"\"\n",
    "        Helper function that uses a logistic function to compute the number of trials.\n",
    "        Models that take less than a second to train will have a large number of trials (150),\n",
    "        while models that take more than 30 seconds will have fewer trials (about 30).\n",
    "        Returns:\n",
    "            int: Computed number of trials.\n",
    "        \"\"\"\n",
    "        single_model_time = self.compute_single_model_time()\n",
    "        print(\"A single model takes \",single_model_time,\"seconds to run\")\n",
    "\n",
    "\n",
    "        return int(30 + (220) / (1 + np.exp(0.2 * (single_model_time - 1))))\n",
    "    \n",
    "\n",
    "\n",
    "    def objective(self, trial):\n",
    "        \"\"\"\n",
    "        An Optuna Objective function, this is the phase 1 function that tunes all 5 parameters each having a wide\n",
    "        range of values. \n",
    "        \"\"\"\n",
    "\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 300, step=50),\n",
    "            'max_depth': trial.suggest_int('max_depth', 10, 30, step = 5),\n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 15),\n",
    "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 15),\n",
    "            'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2'])\n",
    "        }\n",
    "        if self.classification:\n",
    "            model = RandomForestClassifier(**params, n_jobs=-1, random_state=42)\n",
    "            if not self.f1:\n",
    "                score = cross_val_score(model, self.X, self.y, cv=3, n_jobs=-1, scoring='accuracy').mean()\n",
    "            else:\n",
    "                scorer = make_scorer(f1_score, average='weighted')\n",
    "                score = cross_val_score(model, self.X, self.y, cv=3, n_jobs=-1, scoring=scorer).mean()\n",
    "        else:\n",
    "            model = RandomForestRegressor(**params, n_jobs=-1, random_state=42)\n",
    "            score = cross_val_score(model, self.X, self.y, cv=3, n_jobs=-1, scoring='neg_mean_squared_error').mean()\n",
    "\n",
    "        return score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def focused_objective(self, trial):\n",
    "        \"\"\"\n",
    "        A second Optuna Objective function, this is the phase 2 function that tunes only the top 3 parm according to their \n",
    "        importance value determained by Optuna. It then assign a narower set of values for each parameter\n",
    "        \"\"\"\n",
    "\n",
    "        # Sort parameter importances and select the top three\n",
    "        top_params = dict(sorted(self.importance.items(), key=lambda item: item[1], reverse=True)[:3])\n",
    "\n",
    "        params = {}\n",
    "        for param in ['n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf', 'max_features']:\n",
    "            if param in top_params:\n",
    "                base_value = self.best_param[param]\n",
    "                # Adjust the range slightly based on the base_value\n",
    "                if param == 'n_estimators':  # Specific handling for n_estimators\n",
    "                    params[param] = trial.suggest_int(param, max(1, base_value - 50), base_value + 50, step=25)\n",
    "                elif param == 'min_samples_split':  # Specific handling for n_estimators\n",
    "                    params[param] = trial.suggest_int(param, max(2, base_value - 10), base_value + 10)\n",
    "                elif isinstance(base_value, int):\n",
    "                    params[param] = trial.suggest_int(param, max(1, base_value - 5), base_value + 5)\n",
    "                else:  # for categorical parameters like max_features\n",
    "                    params[param] = trial.suggest_categorical(param, ['sqrt', 'log2'])\n",
    "            else:\n",
    "                params[param] = self.best_param[param]\n",
    "\n",
    "        if self.classification:\n",
    "            model = RandomForestClassifier(**params, n_jobs=-1, random_state=42)\n",
    "            if not self.f1:\n",
    "                score = cross_val_score(model, self.X, self.y, cv=3, n_jobs=-1, scoring='accuracy').mean()\n",
    "            else:\n",
    "                scorer = make_scorer(f1_score, average='weighted')\n",
    "                score = cross_val_score(model, self.X, self.y, cv=3, n_jobs=-1, scoring=scorer).mean()\n",
    "        else:\n",
    "            model = RandomForestRegressor(**params, n_jobs=-1, random_state=42)\n",
    "            score = cross_val_score(model, self.X, self.y, cv=3, n_jobs=-1, scoring='neg_mean_squared_error').mean()\n",
    "\n",
    "        return score\n",
    "    \n",
    "\n",
    "\n",
    "    def early_stopping_callback(self, study, trial):\n",
    "        \"\"\"\n",
    "        An optuna callback function, in our case it is used to stop early if no improvamant is made after 0.4*num_trials steps since the\n",
    "        last best score is found or 30 for cases where 0.4*num_trials < 30\n",
    "        \"\"\"\n",
    "        threshold = max(self.num_trials * 0.40, 30)\n",
    "        if study.best_value == self.best_value:\n",
    "            self.trials_since_last_improvement += 1\n",
    "        else:\n",
    "            self.best_value = study.best_value\n",
    "            self.trials_since_last_improvement = 0\n",
    "\n",
    "        if self.trials_since_last_improvement >= threshold:\n",
    "            study.stop()  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def optimize(self):\n",
    "        \"\"\"\n",
    "        The main function that is to be called by the user, it begins phase1, obtain the importance, then start the improved stude\n",
    "        phase 2. finaly it prints the best paramaters alongside the best score, \n",
    "        returns the best params dictionary\n",
    "        Returns:\n",
    "            dic: Best value for each parameter\n",
    "        \"\"\"\n",
    "\n",
    "        ### Phase 1: self.study tunse all 5 paramaters, in a broad set of values\n",
    "\n",
    "        self.study = optuna.create_study(direction='maximize')\n",
    "        self.study.optimize(self.objective, self.num_trials, callbacks=[self.early_stopping_callback])\n",
    "        self.best_param = self.study.best_params                                       # Obtain the best Params\n",
    "        self.importance = optuna.importance.get_param_importances(self.study)          # Obtain the Importance of each param\n",
    "        \n",
    "        \n",
    "        ### Phase 2: using only the top 3 parameters, begin a focused study\n",
    "        self.best_value = float('-inf')                                             \n",
    "        self.trials_since_last_improvement = 0\n",
    "        self.improved_study = optuna.create_study(direction='maximize')\n",
    "        self.improved_study.optimize(self.focused_objective, int(self.num_trials/2), callbacks=[self.early_stopping_callback])\n",
    "\n",
    "        # Update best parameters with improvements from phase 2 if the best score from phase 2 is higher than phase 1 best score\n",
    "        if self.improved_study.best_value > self.study.best_value:\n",
    "            self.best_param.update(self.improved_study.best_params)\n",
    "\n",
    "\n",
    "        print(\"Best Parameters:\", self.best_param)\n",
    "        print(\"Best Score:\", self.improved_study.best_value)\n",
    "        return  self.best_param\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def visualize(self):\n",
    "        \"\"\"\n",
    "        A function that is to be called if the user wish to visualize phase 1 params importance (figure 1)\n",
    "        as well as pararral coordinate of the optimization process for phase 2.\n",
    "        Returns:\n",
    "            tuple: Figures representing parameter importances and parallel coordinates of the optimization process.\n",
    "        \"\"\"\n",
    "        print(\"fig1: After the first phase, This is how important each variable is\")\n",
    "        fig1 = optuna.visualization.plot_param_importances(self.study)\n",
    "        print(\"\")\n",
    "        print(\"fig2: Visualizing the focused study plot parallel coordinate\")\n",
    "        fig2 = optuna.visualization.plot_parallel_coordinate(self.improved_study)\n",
    "        return fig1, fig2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b1def1",
   "metadata": {
    "papermill": {
     "duration": 0.004257,
     "end_time": "2024-11-17T10:34:12.545859",
     "exception": false,
     "start_time": "2024-11-17T10:34:12.541602",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Loading the Data and performing feature engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b33499bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T10:34:12.555744Z",
     "iopub.status.busy": "2024-11-17T10:34:12.555447Z",
     "iopub.status.idle": "2024-11-17T10:34:19.895192Z",
     "shell.execute_reply": "2024-11-17T10:34:19.894200Z"
    },
    "papermill": {
     "duration": 7.347427,
     "end_time": "2024-11-17T10:34:19.897595",
     "exception": false,
     "start_time": "2024-11-17T10:34:12.550168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data = pd.read_csv('/kaggle/input/digitrec/train.csv')\n",
    "\n",
    "X = data.drop('label', axis=1)\n",
    "y = data['label']\n",
    "\n",
    "X_normalized = X / 255.0\n",
    "pca = PCA(n_components=0.90)  \n",
    "X_reduced = pca.fit_transform(X_normalized)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9ef15e",
   "metadata": {
    "papermill": {
     "duration": 0.00438,
     "end_time": "2024-11-17T10:34:19.906699",
     "exception": false,
     "start_time": "2024-11-17T10:34:19.902319",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Testing the performance and efficiency of the PlumberOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "149d7331",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T10:34:19.916667Z",
     "iopub.status.busy": "2024-11-17T10:34:19.916330Z",
     "iopub.status.idle": "2024-11-17T12:28:01.493817Z",
     "shell.execute_reply": "2024-11-17T12:28:01.492773Z"
    },
    "papermill": {
     "duration": 6821.589947,
     "end_time": "2024-11-17T12:28:01.500904",
     "exception": false,
     "start_time": "2024-11-17T10:34:19.910957",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A single model takes  25.327974319458008 seconds to run\n",
      "Suggested number of trails is 31 \n",
      "Best Parameters: {'n_estimators': 300, 'max_depth': 23, 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Best Score: 0.9454761904761906\n",
      "A single model takes  25.271666765213013 seconds to run\n",
      "Suggested number of trails is 31 \n",
      "Best Parameters: {'n_estimators': 300, 'max_depth': 25, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Best Score: 0.9423809523809523\n",
      "A single model takes  25.12181830406189 seconds to run\n",
      "Suggested number of trails is 31 \n",
      "Best Parameters: {'n_estimators': 275, 'max_depth': 35, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Best Score: 0.9438690476190477\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "*************\n",
      "Execution Time: mean=2152.74 seconds, std=268.1613689403954 seconds, max=2432.2526710033417 seconds, min=1791.015058040619 seconds\n",
      "CPU Usage: mean=0.40%, std=0.00%, max=0.40%, min=0.40%\n",
      "Memory Usage: mean=6.12 MB, std=7.94 MB, max=17.35 MB, min=0.21 MB\n",
      "Validation Score: mean=0.9470634920634922 , std=0.0003413621137715419 , max=0.9475 , min=0.9466666666666667 \n"
     ]
    }
   ],
   "source": [
    "# Initialize arrays to store execution time, memory usage, CPU usage, and validation scores for each run of Plumber.\n",
    "plumber_time = np.zeros(3,'float')\n",
    "plumber_mem = np.zeros(3,'float')\n",
    "plumber_cpu =  np.zeros(3,'float')\n",
    "plumber_score = np.zeros(3,'float')\n",
    "# Instantiate the PlumberOptimizer object with the training data for tuning hyperparameters.\n",
    "for i in range(3):\n",
    "    opt = PlumberOptimizer(X_train, y_train)\n",
    "\n",
    "# Capture the system’s initial resource utilization (time, CPU usage, memory) before running the optimizer.\n",
    "    process = psutil.Process(os.getpid())\n",
    "    start_time = time.time()\n",
    "    start_cpu = process.cpu_percent(interval=None)\n",
    "    start_mem = process.memory_info().rss / (1024 * 1024) \n",
    "\n",
    "\n",
    "# Execute the optimization process to find the best hyperparameters.\n",
    "    best = opt.optimize()\n",
    "\n",
    "# Capture resource utilization after running the optimizer.\n",
    "    end_time = time.time()\n",
    "    end_cpu = process.cpu_percent(interval=None)\n",
    "    end_mem = process.memory_info().rss / (1024 * 1024)  \n",
    "\n",
    "\n",
    "# Calculate and store the differences in resource utilization metrics for this run.\n",
    "    plumber_time[i] = end_time - start_time\n",
    "    plumber_mem[i] = end_mem - start_mem\n",
    "    plumber_cpu[i] = end_cpu - start_cpu\n",
    "\n",
    "# Train the Random Forest model with the optimized hyperparameters and compute the accuracy score for evaluation.\n",
    "    rf = RandomForestClassifier(**best , random_state=42 )\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = rf.predict(X_test)\n",
    "\n",
    "\n",
    "    plumber_score[i] = accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(\"\\n\" *3)\n",
    "print(\"*************\")\n",
    "print(f\"Execution Time: mean={np.mean(plumber_time):.2f} seconds, std={np.std(plumber_time)} seconds, max={np.max(plumber_time)} seconds, min={np.min(plumber_time)} seconds\")\n",
    "print(f\"CPU Usage: mean={np.mean(plumber_cpu):.2f}%, std={np.std(plumber_cpu):.2f}%, max={np.max(plumber_cpu):.2f}%, min={np.min(plumber_cpu):.2f}%\")\n",
    "print(f\"Memory Usage: mean={np.mean(plumber_mem):.2f} MB, std={np.std(plumber_mem):.2f} MB, max={np.max(plumber_mem):.2f} MB, min={np.min(plumber_mem):.2f} MB\")\n",
    "print(f\"Validation Score: mean={np.mean(plumber_score)} , std={np.std(plumber_score)} , max={np.max(plumber_score)} , min={np.min(plumber_score)} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1d414e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T12:28:01.512952Z",
     "iopub.status.busy": "2024-11-17T12:28:01.512585Z",
     "iopub.status.idle": "2024-11-17T12:28:01.516736Z",
     "shell.execute_reply": "2024-11-17T12:28:01.515857Z"
    },
    "papermill": {
     "duration": 0.012418,
     "end_time": "2024-11-17T12:28:01.518739",
     "exception": false,
     "start_time": "2024-11-17T12:28:01.506321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fig1, fig2 = opt.visualize()\n",
    "\n",
    "# fig1.show()\n",
    "# fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66697bbb",
   "metadata": {
    "papermill": {
     "duration": 0.004753,
     "end_time": "2024-11-17T12:28:01.528482",
     "exception": false,
     "start_time": "2024-11-17T12:28:01.523729",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Testing the performance and efficiency of the GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83e291cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T12:28:01.539862Z",
     "iopub.status.busy": "2024-11-17T12:28:01.539516Z",
     "iopub.status.idle": "2024-11-17T14:04:07.133189Z",
     "shell.execute_reply": "2024-11-17T14:04:07.132089Z"
    },
    "papermill": {
     "duration": 5765.607005,
     "end_time": "2024-11-17T14:04:07.140404",
     "exception": false,
     "start_time": "2024-11-17T12:28:01.533399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: 5667.087579488754 seconds\n",
      "CPU Usage: 2.0%\n",
      "Memory Usage: 97.40625 MB\n",
      "Validation Accuracy: 0.9477380952380953\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter grid for Grid Search.\n",
    "param_grid = {\n",
    "    'n_estimators': [200, 400],  \n",
    "    'max_features': ['sqrt', 'log2'], \n",
    "    'max_depth': [ 10, 30, 50], \n",
    "    'min_samples_split': [2, 8, 14],  \n",
    "    'min_samples_leaf': [2,  8,  14]  \n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Set up Grid Search with cross-validation and accuracy as the evaluation metric.\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, scoring='accuracy')\n",
    "\n",
    "# Capture initial resource usage metrics for Grid Search.\n",
    "process = psutil.Process(os.getpid())\n",
    "start_time = time.time()\n",
    "start_cpu = process.cpu_percent(interval=None)\n",
    "start_mem = process.memory_info().rss / (1024 * 1024)  \n",
    "\n",
    "# Perform hyperparameter tuning using Grid Search.\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Record resource utilization after tuning.\n",
    "end_time = time.time()\n",
    "end_cpu = process.cpu_percent(interval=None)\n",
    "end_mem = process.memory_info().rss / (1024 * 1024)  \n",
    "\n",
    "print(f\"Execution Time: {end_time - start_time} seconds\")\n",
    "print(f\"CPU Usage: {end_cpu - start_cpu}%\")\n",
    "print(f\"Memory Usage: {end_mem - start_mem} MB\")\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(** grid_search.best_params_, random_state=42 )\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "\n",
    "# Compute the validation accuracy of the best model obtained from Grid Search.\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5871172d",
   "metadata": {
    "papermill": {
     "duration": 0.005187,
     "end_time": "2024-11-17T14:04:07.151064",
     "exception": false,
     "start_time": "2024-11-17T14:04:07.145877",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Testing the performance and efficiency of the RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21970a98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T14:04:07.163275Z",
     "iopub.status.busy": "2024-11-17T14:04:07.162923Z",
     "iopub.status.idle": "2024-11-17T15:54:58.269857Z",
     "shell.execute_reply": "2024-11-17T15:54:58.268877Z"
    },
    "papermill": {
     "duration": 6651.122403,
     "end_time": "2024-11-17T15:54:58.278813",
     "exception": false,
     "start_time": "2024-11-17T14:04:07.156410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: mean=2159.84 seconds, std=73.73932292993157 seconds, max=2258.1127712726593 seconds, min=2080.481866121292 seconds\n",
      "CPU Usage: mean=2.90%, std=0.65%, max=3.50%, min=2.00%\n",
      "Memory Usage: mean=2.88 MB, std=1.50 MB, max=4.98 MB, min=1.53 MB\n",
      "Validation Score: mean=0.9446428571428571 , std=0.0013712025827913369 , max=0.9461904761904761 , min=0.9428571428571428 \n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter search space for Random Search.\n",
    "param_dist = {\n",
    "    'n_estimators': np.arange(50, 500),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': np.arange(10, 50),\n",
    "    'min_samples_split': np.arange(2, 30),\n",
    "    'min_samples_leaf': np.arange(1, 30)\n",
    "}\n",
    "# Initialize arrays to store metrics for multiple runs of Random Search.\n",
    "rand_time = np.zeros(3,'float')\n",
    "rand_mem = np.zeros(3,'float')\n",
    "rand_cpu =  np.zeros(3,'float')\n",
    "rand_score = np.zeros(3,'float')\n",
    "# Perform Random Search multiple times to evaluate its performance.\n",
    "for i in range(3):\n",
    "    # Instantiate the RandomForestClassifier model.\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    # Set up Random Search with 45 iterations and cross-validation.\n",
    "    random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist,\n",
    "                                   n_iter=45, cv=3, n_jobs=-1)\n",
    "    # Capture initial resource usage before running Random Search.\n",
    "    process = psutil.Process(os.getpid())\n",
    "    start_time = time.time()\n",
    "    start_cpu = process.cpu_percent(interval=None)\n",
    "    start_mem = process.memory_info().rss / (1024 * 1024)  \n",
    "\n",
    "    # Perform Random Search to tune hyperparameters.\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    # Capture resource usage after running Random Search.\n",
    "    end_time = time.time()\n",
    "    end_cpu = process.cpu_percent(interval=None)\n",
    "    end_mem = process.memory_info().rss / (1024 * 1024)  \n",
    "\n",
    "\n",
    "    # Store the resource usage metrics for this run.\n",
    "    rand_time[i] = end_time - start_time\n",
    "    rand_mem[i] = end_mem - start_mem\n",
    "    rand_cpu[i] = end_cpu - start_cpu\n",
    "\n",
    "    # Train and evaluate the model with the best parameters from Random Search.\n",
    "    rf = RandomForestClassifier(** random_search.best_params_ , random_state=42 )\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "\n",
    "\n",
    "    rand_score[i] = accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "# Print summary statistics for resource usage and validation scores across all runs.\n",
    "print(f\"Execution Time: mean={np.mean(rand_time):.2f} seconds, std={np.std(rand_time)} seconds, max={np.max(rand_time)} seconds, min={np.min(rand_time)} seconds\")\n",
    "print(f\"CPU Usage: mean={np.mean(rand_cpu):.2f}%, std={np.std(rand_cpu):.2f}%, max={np.max(rand_cpu):.2f}%, min={np.min(rand_cpu):.2f}%\")\n",
    "print(f\"Memory Usage: mean={np.mean(rand_mem):.2f} MB, std={np.std(rand_mem):.2f} MB, max={np.max(rand_mem):.2f} MB, min={np.min(rand_mem):.2f} MB\")\n",
    "print(f\"Validation Score: mean={np.mean(rand_score)} , std={np.std(rand_score)} , max={np.max(rand_score)} , min={np.min(rand_score)} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255413d8",
   "metadata": {
    "papermill": {
     "duration": 0.005217,
     "end_time": "2024-11-17T15:54:58.289482",
     "exception": false,
     "start_time": "2024-11-17T15:54:58.284265",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Testing the performance and efficiency of the BayesSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f99c2660",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T15:54:58.301425Z",
     "iopub.status.busy": "2024-11-17T15:54:58.301084Z",
     "iopub.status.idle": "2024-11-17T18:31:13.429662Z",
     "shell.execute_reply": "2024-11-17T18:31:13.428559Z"
    },
    "papermill": {
     "duration": 9375.143751,
     "end_time": "2024-11-17T18:31:13.438490",
     "exception": false,
     "start_time": "2024-11-17T15:54:58.294739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: mean=2996.16 seconds, std=140.05438087838198 seconds, max=3157.4255545139313 seconds, min=2815.938412666321 seconds\n",
      "CPU Usage: mean=10.57%, std=0.59%, max=11.40%, min=10.10%\n",
      "Memory Usage: mean=421.02 MB, std=10.20 MB, max=435.16 MB, min=411.49 MB\n",
      "Validation Score: mean=0.9496825396825397 , std=0.00020234204419021528 , max=0.9498809523809524 , min=0.9494047619047619 \n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter search space for Bayesian Search.\n",
    "param_dist = {\n",
    "    'n_estimators': Integer(50, 500),\n",
    "    'max_features': Categorical(['sqrt', 'log2']),\n",
    "    'max_depth': Integer(10, 50),\n",
    "    'min_samples_split': Integer(2, 30),\n",
    "    'min_samples_leaf': Integer(1, 30)\n",
    "}\n",
    "\n",
    "# Initialize arrays to store metrics for multiple runs of Bayesian Search.\n",
    "rand_time = np.zeros(3,'float')\n",
    "rand_mem = np.zeros(3,'float')\n",
    "rand_cpu =  np.zeros(3,'float')\n",
    "rand_score = np.zeros(3,'float')\n",
    "# Perform Bayesian Search multiple times to evaluate its performance.\n",
    "for i in range(3):\n",
    "    # Instantiate the RandomForestClassifier model.\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    # Set up Bayesian Search with 40 iterations and cross-validation.\n",
    "    bayes_search = BayesSearchCV(estimator=rf, search_spaces=param_dist,\n",
    "                                 n_iter=40, cv=3, n_jobs=-1)\n",
    "    # Capture initial resource usage before running Bayesian Search.\n",
    "    process = psutil.Process(os.getpid())\n",
    "    start_time = time.time()\n",
    "    start_cpu = process.cpu_percent(interval=None)\n",
    "    start_mem = process.memory_info().rss / (1024 * 1024) \n",
    "\n",
    "    # Perform Bayesian Search to tune hyperparameters.\n",
    "    bayes_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Capture resource usage after running Bayesian Search.\n",
    "    end_time = time.time()\n",
    "    end_cpu = process.cpu_percent(interval=None)\n",
    "    end_mem = process.memory_info().rss / (1024 * 1024)  \n",
    "\n",
    "    # Store the resource usage metrics for this run.\n",
    "    rand_time[i] = end_time - start_time\n",
    "    rand_mem[i] = end_mem - start_mem\n",
    "    rand_cpu[i] = end_cpu - start_cpu\n",
    "\n",
    "    # Train and evaluate the model with the best parameters from Bayesian Search.\n",
    "    rf = RandomForestClassifier(** bayes_search.best_params_ , random_state=42 )\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    rand_score[i] = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print summary statistics for resource usage and validation scores across all runs.\n",
    "print(f\"Execution Time: mean={np.mean(rand_time):.2f} seconds, std={np.std(rand_time)} seconds, max={np.max(rand_time)} seconds, min={np.min(rand_time)} seconds\")\n",
    "print(f\"CPU Usage: mean={np.mean(rand_cpu):.2f}%, std={np.std(rand_cpu):.2f}%, max={np.max(rand_cpu):.2f}%, min={np.min(rand_cpu):.2f}%\")\n",
    "print(f\"Memory Usage: mean={np.mean(rand_mem):.2f} MB, std={np.std(rand_mem):.2f} MB, max={np.max(rand_mem):.2f} MB, min={np.min(rand_mem):.2f} MB\")\n",
    "print(f\"Validation Score: mean={np.mean(rand_score)} , std={np.std(rand_score)} , max={np.max(rand_score)} , min={np.min(rand_score)} \")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6086915,
     "sourceId": 9907351,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 28628.569236,
   "end_time": "2024-11-17T18:31:16.163629",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-17T10:34:07.594393",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
